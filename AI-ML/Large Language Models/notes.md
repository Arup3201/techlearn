# Generative AI and Large Language Models

## Introduction

Large language models or LLMs are a very exciting technology. But despite all the buzz and hype, one of the thing that is still underestimated by many people is their power as a developer tool.

Specifically, there are many machine learning and AI applications that used to take many months to build that you can now build in days or maybe even small numbers of weeks. This note will take a deep dive with you into how LLM technology actually works including going through many of the technical details, like model training, instruction tuning, fine-tuning, the generative AI project life cycle framework to help you plan and execute your projects and so on.

Generative AI and LLMs specifically are a general purpose technology. That means that similar to other general purpose technologies like deep learning and electricity, is useful not just for a single application, but for a lot of different applications that span many corners of the economy.

The first section will share with you a deep dive into how transformer networks actually work. In 2017, the paper came out, Attention is all You Need, and it laid out all of these fairly complex data processes which are going to happen inside the transformer architecture. So we take a little bit of a high level view, but we do go down into some depths. We talk about things like self-attention and the multi-headed self-attention mechanism. So we can see why it is that these models actually work, how it is that they actually gain an understanding of language.

It's been really interesting to see how that basic transformer architecture is creating a foundation for vision transformers as well. Understanding transformers is also helping people understand this really exciting vision transformer and other modalities as well. It's going to be a really critical building block for a lot of machine learning.

Then comes the Generative AI project Lifecycle which walks you through the individual stages and decisions you have to make when you're developing Generative AI applications. So one of the first things you have to decide is whether you're taking a foundation model off the shelf or you're actually pre-training your own model and then as a follow up, whether you want to fine tune and customize that model maybe for your specific data.

There might be use cases where you really need the model to be very comprehensive and able to generalize to a lot of different tasks. And there might be use cases where you're just optimizing for a single-use case, right? And you can potentially work with a smaller model and achieving similar or even very good results. When you want your large language model to have a lot of general knowledge about the world, when you wanted to know stuff about history and philosophy and the sizes and how to write Python code and so on and so on. It helps to have a giant model with hundreds of billions of parameters. But for a single task like summarizing dialogue or acting as a customer service agent for one company, for applications like that, sometimes you can use hundreds of billions of parameters models. But that's not always necessary.

We'll talk about large language models, their use cases, how the models work, prompt engineering, how to make creative text outputs, and outline a project lifecycle for generative AI projects.

## Foundation models

Whether it be a chat bot, generating images from text, or using a plugin to help you develop code, what you see in these tools is a machine that is capable of creating content that mimics or approximates human ability.

Generative AI is a subset of traditional machine learning. And the machine learning models that underpin generative AI have learned these abilities by finding statistical patterns in massive datasets of content that was originally generated by humans. Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power. These foundation models, as we call them, with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve.

![Foundation models](./images/foundation-models.png)

Here are a collection of foundation models, sometimes called base models, and their relative size in terms of their parameters. You can think of them as the model's memory. And the more parameters a model has, the more memory, and as it turns out, the more sophisticated the tasks it can perform.

By either using these models as they are or by applying fine tuning techniques to adapt them to your specific use case, you can rapidly build customized solutions without the need to train a new model from scratch.

Now, while generative AI models are being created for multiple modalities, including images, video, audio, and speech, you'll focus on large language models and their uses in natural language generation. You will see how they are built and trained, how you can interact with them via text known as prompts. And how to fine tune models for your use case and data, and how you can deploy them with applications to solve your business and social tasks.

The way you interact with language models is quite different than other machine learning and programming paradigms. In those cases, you write computer code with formalized syntax to interact with libraries and APIs.

In contrast, large language models are able to take natural language or human written instructions and perform tasks much as a human would. The text that you pass to an LLM is known as a `prompt`. The space or memory that is available to the prompt is called the `context window`, and this is typically large enough for a few thousand words, but differs from model to model.

In this example, you ask the model to determine `where Ganymede is located in the solar system?`. The prompt is passed to the model, the model then predicts the next words, and because your prompt contained a question, this model generates an answer. The output of the model is called a `completion`, and the act of using the model to generate text is known as `inference`. The completion is comprised of the text contained in the original prompt, followed by the generated text.

![Promts and completions](./images/prompts-completions.png)

## Use cases of LLMs

LLMs and generative AI are focused on chats tasks. After all, chatbots are highly visible and getting a lot of attention. Next word prediction is the base concept behind a number of different capabilities, starting with a basic chatbot.

However, you can use this conceptually simple technique for a variety of other tasks within text generation.

For example, you can ask a model to write an essay based on a prompt, to summarize conversations where you provide the dialogue as part of your prompt and the model uses this data along with its understanding of natural language to generate a summary.

You can use models for a variety of translation tasks from traditional translation between two different languages, such as French and German, or English and Spanish.

Or to translate natural language to machine code. For example, you could ask a model to "write some Python code that will return the mean of every column in a DataFrame" and the model will generate code that you can pass to an interpreter.

You can use LLMs to carry out smaller, focused tasks like information retrieval. In this example, you ask the model to identify all of the people and places identified in a news article. This is known as `named entity recognition`, a word classification. The understanding of knowledge encoded in the model's parameters allows it to correctly carry out this task and return the requested information to you.

Finally, an area of active development is augmenting LLMs by connecting them to external data sources or using them to invoke external APIs. You can use this ability to provide the model with information it doesn't know from its pre-training and to enable your model to power interactions with the real-world.

Developers have discovered that as the scale of foundation models grows from hundreds of millions of parameters to billions, even hundreds of billions, the subjective understanding of language that a model possesses also increases.

This language understanding stored within the parameters of the model is what processes, reasons, and ultimately solves the tasks you give it, but it's also true that smaller models can be fine tuned to perform well on specific focused tasks.

## Text Generation before transformers

It's important to note that generative algorithms are not new. Previous generations of language models made use of an architecture called recurrent neural networks or RNNs.

RNNs while powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks. Let's look at an example of an RNN carrying out a simple next-word prediction generative task.

```
...tastes ?
...tea tastes ?
..., my tea tastes (great) - Prediction
[The milk was bad], so the tea tastes (great) X
```

With just one previous words seen by the model, the prediction can't be very good. As you scale the RNN implementation to be able to see more of the preceding words in the text, you have to significantly scale the resources that the model uses. As for the prediction, well, the model failed here.

Even though you scale the model, it still hasn't seen enough of the input to make a good prediction. To successfully predict the next word, models need to see more than just the previous few words. Models needs to have an understanding of the whole sentence or even the whole document.

The problem here is that language is complex. In many languages, one word can have multiple meanings. These are homonyms.

```
I took my money to (bank).
```

In this case, it's only with the context of the sentence that we can see what kind of bank is meant.

Words within a sentence structures can be ambiguous or have what we might call syntactic ambiguity. Take for example this sentence,

```
The teacher taught the students with the book.
```

Did the teacher teach using the book or did the student have the book, or was it both? How can an algorithm make sense of human language if sometimes we can't?

In 2017, after the publication of this paper, `Attention is All You Need`, from Google and the University of Toronto, everything changed. The transformer architecture had arrived. This novel approach unlocked the progress in generative AI that we see today.

It can be scaled efficiently to use multi-core GPUs, it can parallel process input data, making use of much larger training datasets, and crucially, it's able to learn to pay attention to the meaning of the words it's processing. And attention is all you need.

## Transformers

Building large language models using the transformer architecture dramatically improved the performance of natural language tasks over the earlier generation of RNNs, and led to an explosion in regenerative capability.

The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence. Not just as you see here, to each word next to its neighbor, but to every other word in a sentence.

![Transformers self-attention on sentence](./images/transformer-working-sentence.png)

To apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn who has the book, who could have the book, and if it's even relevant to the wider context of the document.

These attention weights are learned during LLM training and you'll learn more about this later this week. This diagram is called an attention map and can be useful to illustrate the attention weights between each word and every other word. Here in this stylized example, you can see that the word book is strongly connected with or paying attention to the word teacher and the word student. This is called self-attention and the ability to learn attension in this way across the whole input significantly approves the model's ability to encode language.

![Attention map of transformer](./images/transformer-attention-map.png)

Now that you've seen one of the key attributes of the transformer architecture, self-attention, let's cover at a high level how the model works.

![Simplified view of transformer architecture](./images/simplified-transformer-architecture.png)

Here's a simplified diagram of the transformer architecture so that you can focus at a high level on where these processes are taking place.

The transformer architecture is split into two distinct parts, the encoder and the decoder.

These components work in conjunction with each other and they share a number of similarities.

Also, note here, the diagram you see is derived from the original attention is all you need paper. Notice how the inputs to the model are at the bottom and the outputs are at the top.

Now, machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words.

Simply put, this converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with.

You can choose from multiple tokenization methods. For example, token IDs matching to complete words, or using token IDs to represent parts of words. What's important is that once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text.

Now that your input is represented as numbers, you can pass it to the embedding layer.

This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space.

Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence.

Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.

Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector.

![Transformer from tokenizer to embedding layer](./images/transformer-tokenizer-embedding.png)

In the original transformer paper, the vector size was actually 512, so much bigger than we can fit onto this image. For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.

![Vector space for embedding vectors](./images/word-embedding-vector-space.png)

You can see now how you can relate words that are located close to each other in the embedding space, and how you can calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language.

As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding. The model processes each of the input tokens in parallel. So by adding the positional encoding, you preserve the information about the word order and don't lose the relevance of the position of the word in the sentence.

![Transformer embedding output added to positional encoding](./images/transformer-embedding-positional.png)

Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer. Here, the model analyzes the relationships between the tokens in your input sequence.

As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.

The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence.

But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other.

The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.

The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme.

It's important to note that you don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.

![Transformer multi-headed self-attention](./images/transformer-multi-headed-self-attention.png)

While some attention maps are easy to interpret, like the examples discussed here, others may not be.

Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network.

The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary.

![Transformer feed forward](./images/transformer-feed-forward.png)

You can then pass these logits to a final softmax layer, where they are normalized into a probability score for each word.

This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token.

![Transformer softmax layer](./images/transformer-softmax-layer.png)
